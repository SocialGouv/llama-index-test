{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievers and Node Post-Processors\n",
    "\n",
    "In this notebook, we cover some customization to our existing retrieval process, using the `HierarchicalNodeParser`, `AutoMergingRetriever`, \n",
    "and a custom node-postprocessor that ensures a certain amount of tokens are always sent to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR KEY\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Use local embeddings + gpt-3.5-turbo-16k\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-16k\", max_tokens=512, temperature=0.1),\n",
    "    # embed_model=\"local:BAAI/bge-base-en\"\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Parsing + Retrieval\n",
    "\n",
    "Previously, we used a custom markdown loader to load chunks from our markdown documentation. However, since then, advancements have been made in llama-index that may provide more relevant retrieval. Specifically, we will use the `HierarchicalNodeParser`, which parses nodes into several chunk sizes.\n",
    "\n",
    "The idea here is that during retrieval, if a majority of chunks are retrieved that have the same parent chunk, we return the larger parent chunk instead.\n",
    "\n",
    "To support this, we can modify our loading code as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, Document\n",
    "from llama_index.node_parser import HierarchicalNodeParser, SimpleNodeParser, get_leaf_nodes\n",
    "from llama_index.schema import MetadataMode\n",
    "from llama_docs_bot.markdown_docs_reader import MarkdownDocsReader\n",
    "\n",
    "\n",
    "def load_markdown_docs(filepath, hierarchical=True):\n",
    "    \"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath, \n",
    "        required_exts=[\".md\"],\n",
    "        file_extractor={\".md\": MarkdownDocsReader()},\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "    if hierarchical:\n",
    "        # combine all documents into one\n",
    "        documents = [\n",
    "            Document(text=\"\\n\\n\".join(\n",
    "                    document.get_content(metadata_mode=MetadataMode.ALL) \n",
    "                    for document in documents\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # chunk into 3 levels\n",
    "        # majority means 2/3 are retrieved before using the parent\n",
    "        large_chunk_size = 1536\n",
    "        node_parser = HierarchicalNodeParser.from_defaults(\n",
    "            chunk_sizes=[\n",
    "                large_chunk_size, \n",
    "                large_chunk_size // 3,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        return nodes, get_leaf_nodes(nodes)\n",
    "    else:\n",
    "        node_parser = SimpleNodeParser.from_defaults()\n",
    "        nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we parse each directory into a single giant document, and then chunk into a heirarchy of 2048, 2048 // 3, and 2048 // 9. \n",
    "\n",
    "This means if 2 of 3 child chunks are retrieved, the `AutoMergingRetriever` will replace the nodes with the larger parent chunk.\n",
    "\n",
    "Now, in order for the auto merging to work properly, we will need to set the top-k higher. However, we still want to avoid sending too much text to the LLM for the sake of latency. So here, we also introduce a local re-ranker to limit the amount of returned nodes after merging.\n",
    "\n",
    "### Load/Create Query Engines\n",
    "\n",
    "Let's write a function to build our query engine tools next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex,StorageContext, load_index_from_storage\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import AutoMergingRetriever\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "\n",
    "def get_query_engine_tool(directory, description, hierarchical=True, postprocessors=None):\n",
    "    try:\n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            persist_dir=f\"./data_{os.path.basename(directory)}\"\n",
    "        )\n",
    "        index = load_index_from_storage(storage_context)\n",
    "\n",
    "        if hierarchical:\n",
    "            retriever = AutoMergingRetriever(\n",
    "                index.as_retriever(similarity_top_k=6), \n",
    "                storage_context=storage_context\n",
    "            )\n",
    "        else:\n",
    "            retriever = index.as_retriever(similarity_top_k=12)\n",
    "    except:\n",
    "        if hierarchical:\n",
    "            nodes, leaf_nodes = load_markdown_docs(directory, hierarchical=hierarchical)\n",
    "\n",
    "            docstore = SimpleDocumentStore()\n",
    "            docstore.add_documents(nodes)\n",
    "            storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "            index = VectorStoreIndex(leaf_nodes, storage_context=storage_context)\n",
    "            index.storage_context.persist(persist_dir=f\"./data_{os.path.basename(directory)}\")\n",
    "\n",
    "            retriever = AutoMergingRetriever(\n",
    "                index.as_retriever(similarity_top_k=12), \n",
    "                storage_context=storage_context\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            nodes = load_markdown_docs(directory, hierarchical=hierarchical)\n",
    "            index = VectorStoreIndex(nodes)\n",
    "            index.storage_context.persist(persist_dir=f\"./data_{os.path.basename(directory)}\")\n",
    "\n",
    "            retriever = index.as_retriever(similarity_top_k=12)\n",
    "\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever,\n",
    "        node_postprocessors=postprocessors or [],\n",
    "    )\n",
    "\n",
    "    return QueryEngineTool(query_engine=query_engine, metadata=ToolMetadata(name=directory, description=description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare retrievers\n",
    "\n",
    "You'll notice we included some code to enable/disable the hierarchical node parsing. Let's compare results a bit quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_engine = get_query_engine_tool(\n",
    "    \"../docs/core_modules/query_modules\",\n",
    "    \"Useful for information on various query engines and retrievers, and anything related to querying data.\",\n",
    "    hierarchical=True, \n",
    ").query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_engine = get_query_engine_tool(\n",
    "    \"../docs/core_modules/query_modules\",\n",
    "    \"Useful for information on various query engines and retrievers, and anything related to querying data.\",\n",
    "    hierarchical=False, \n",
    ").query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "qb = QueryBundle(\"How do I setup a query engine?\")\n",
    "hierarchical_nodes = hierarchical_engine.retrieve(qb)\n",
    "base_nodes = base_engine.retrieve(QueryBundle(\"How do I setup a query engine?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hierarchical ---\n",
      "\n",
      "Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "A query engine takes in a natural language query, and returns a rich response.\n",
      "It is most often (but not always) built on one or many Indices via Retrievers.\n",
      "You can compose multiple query engines to achieve more advanced capability.\n",
      "---\n",
      "You can directly build and configure a query engine from an index in 1 line of code:\n",
      "---\n",
      "If you want to ask standalone question over your data (i.e. without keeping track of conversation history), use Query Engine instead.\n",
      "---\n",
      "Build a query engine from index:\n",
      "---\n",
      "Get started with:\n",
      "---\n",
      "To enable streaming, you need to use an LLM that supports streaming.\n",
      "Right now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).\n",
      "\n",
      "Configure query engine to use streaming:\n",
      "\n",
      "If you are using the high-level API, set `streaming=True` when building a query engine.\n",
      "Total length: 468\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils import globals_helper\n",
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "print(\"\\n--- Hierarchical ---\\n\")\n",
    "print('\\n---\\n'.join([node.node.text for node in hierarchical_nodes]))\n",
    "\n",
    "total_length = 0\n",
    "for node in hierarchical_nodes:\n",
    "    total_length += len(globals_helper.tokenizer(node.node.get_content(metadata_mode=MetadataMode.LLM)))\n",
    "print(f\"Total length: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Base ---\n",
      "\n",
      "Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "A query engine takes in a natural language query, and returns a rich response.\n",
      "It is most often (but not always) built on one or many Indices via Retrievers.\n",
      "You can compose multiple query engines to achieve more advanced capability.\n",
      "---\n",
      "You can directly build and configure a query engine from an index in 1 line of code:\n",
      "---\n",
      "If you want to ask standalone question over your data (i.e. without keeping track of conversation history), use Query Engine instead.\n",
      "---\n",
      "Build a query engine from index:\n",
      "---\n",
      "Get started with:\n",
      "---\n",
      "query_engine = RetrieverQueryEngine(\n",
      "    retriever=retriever,\n",
      "    response_synthesizer=response_synthesizer,\n",
      ")\n",
      "---\n",
      "To enable streaming, you need to use an LLM that supports streaming.\n",
      "Right now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).\n",
      "\n",
      "Configure query engine to use streaming:\n",
      "\n",
      "If you are using the high-level API, set `streaming=True` when building a query engine.\n",
      "---\n",
      "response = query_engine.query(\"What did the author do growing up?\")\n",
      "print(response)\n",
      "---\n",
      "query_engine = index.as_query_engine(\n",
      "    streaming=True,\n",
      ")\n",
      "streaming_response = query_engine.query(\n",
      "    \"What did the author do growing up?\", \n",
      ")\n",
      "streaming_response.print_response_stream()\n",
      "---\n",
      "If you are using the low-level API to compose the query engine,\n",
      "pass `streaming=True` when constructing the `Response Synthesizer`:\n",
      "---\n",
      "If you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine\n",
      "---\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "Total length: 949\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Base ---\\n\")\n",
    "print('\\n---\\n'.join([node.node.text for node in base_nodes]))\n",
    "\n",
    "total_length = 0\n",
    "for node in base_nodes:\n",
    "    total_length += len(globals_helper.tokenizer(node.node.get_content(metadata_mode=MetadataMode.LLM)))\n",
    "print(f\"Total length: {total_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the hierarchical query engine seems to return better text, but there is also a LOT of text.\n",
    "\n",
    "If not enough nodes are merged in the retriever, we can end up with a lot of text, due to setting the top-k so high.\n",
    "\n",
    "So, let's write a custom node-postprocessor to make sure this doesn't happen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Node Post-Processor\n",
    "\n",
    "Here, we use a very basic approach to approximate token counts. We return the most nodes that fit within our token count.\n",
    "The nodes are already pre-sorted, so we don't have to worry about similarity scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "from llama_index.utils import globals_helper\n",
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "class LimitRetrievedNodesLength:\n",
    "\n",
    "    def __init__(self, limit: int = 3000, tokenizer: Optional[Callable] = None):\n",
    "        self._tokenizer = tokenizer or globals_helper.tokenizer\n",
    "        self.limit = limit\n",
    "\n",
    "    def postprocess_nodes(self, nodes, query_bundle):\n",
    "        included_nodes = []\n",
    "        current_length = 0\n",
    "\n",
    "        for node in nodes:\n",
    "            current_length += len(self._tokenizer(node.node.get_content(metadata_mode=MetadataMode.LLM)))\n",
    "            if current_length > self.limit:\n",
    "                break\n",
    "            included_nodes.append(node)\n",
    "\n",
    "        return included_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_query_engine_tool(\n",
    "    \"../docs/core_modules/query_modules\",\n",
    "    \"Useful for information on various query engines and retrievers, and anything related to querying data.\",\n",
    "    hierarchical=True,\n",
    "    postprocessors=[LimitRetrievedNodesLength(limit=3000)]\n",
    ").query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 1868\n"
     ]
    }
   ],
   "source": [
    "hierarchical_nodes = query_engine.retrieve(QueryBundle(\"How do I setup a query engine?\"))\n",
    "total_length = 0\n",
    "for node in hierarchical_nodes:\n",
    "    total_length += len(globals_helper.tokenizer(node.node.get_content(metadata_mode=MetadataMode.LLM)))\n",
    "print(f\"Total length: {total_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our functions setup, we can load/create our indexes and create our final query engine across our documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.query_engine import SubQuestionQueryEngine, RouterQueryEngine\n",
    "\n",
    "# Here we define the directories we want to index, as well as a description for each\n",
    "# NOTE: these descriptions are hand-written based on my understanding. We could have also\n",
    "# used an LLM to write these, maybe a future experiment.\n",
    "docs_directories = {\n",
    "  #  \"../docs/community\": \"Useful for information on community integrations with other libraries, vector dbs, and frameworks.\", \n",
    "  #  \"../docs/core_modules/agent_modules\": \"Useful for information on data agents and tools for data agents.\", \n",
    "  #  \"../docs/core_modules/data_modules\": \"Useful for information on data, storage, indexing, and data processing modules.\",\n",
    "  #  \"../docs/core_modules/model_modules\": \"Useful for information on LLMs, embedding models, and prompts.\",\n",
    " #   \"../docs/core_modules/query_modules\": \"Useful for information on various query engines and retrievers, and anything related to querying data.\",\n",
    "  #  \"../docs/core_modules/supporting_modules\": \"Useful for information on supporting modules, like callbacks, evaluators, and other supporting modules.\",\n",
    "   # \"../docs/getting_started\": \"Useful for information on getting started with LlamaIndex.\", \n",
    "   # \"../docs/development\": \"Useful for information on contributing to LlamaIndex development.\",\n",
    "    \"../content/standup-fabrique\": \"Pour consulter l'actualité et les chiffres d'une startup.\",\n",
    "    \"../content/support-sre-fabrique\": \"Pour les questions techniques et développement et déploiement.\"\n",
    "}\n",
    "\n",
    "# Build query engine tools\n",
    "query_engine_tools = [\n",
    "    get_query_engine_tool(\n",
    "        directory, \n",
    "        description, \n",
    "        hierarchical=True, \n",
    "        postprocessors=[LimitRetrievedNodesLength(limit=3000)]\n",
    "    ) for directory, description in docs_directories.items()\n",
    "]\n",
    "\n",
    "# build top-level router -- this will route to multiple sub-indexes and aggregate results\n",
    "# query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "#     query_engine_tools=query_engine_tools,\n",
    "#     service_context=service_context,\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "query_engine = RouterQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    select_multi=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Voici les dernières nouvelles des startups :\n",
       "\n",
       "1. Startup MANO :\n",
       "   - Nombre d'organisations : 269\n",
       "   - Nombre de personnes suivies : 158 000\n",
       "   - Nombre de professionnels utilisateurs : 2400\n",
       "   - Nombre d'équipes : 743\n",
       "\n",
       "2. Startup MonPsy :\n",
       "   - MonPsy devient MonParcoursPsy\n",
       "   - Organisation du changement de nom : répercussion sur le site, la démarche DS, les supports de communication, de conventionnement, etc\n",
       "   - Intégration d'un encart réciproque MonPsy / Santé PsyEtudiants sur nos sites respectifs\n",
       "   - Départ d'Audrey et Laëtitia à fin novembre\n",
       "   - Candidatures Psy : 2615\n",
       "   - Psy partenaires : 2052 / 4000\n",
       "   - Patients suivis au 31/10 : 52 186 / 60 000\n",
       "\n",
       "3. Startup NATA :\n",
       "   - Après la saison estivale ...\n",
       "   - La phase d'expérimentation sur la cible grande précarité (rue/hôtels sociaux) n'est pas concluante malgré de fortes actions de déploiement en faveur des acteurs terrains\n",
       "   - Un pivot urgent sur l'élargissement de la cible est en cours avec des actions auprès des sages-femmes/Assistantes sociales et PMIS et ce dans toute l'Ile de France.\n",
       "   - Produit : l'application a de nouvelles fonctionnalités,\n",
       "     -> Modification de son profil avec une mise à jour automatique des ressources et rdvs ciblés\n",
       "     -> Intégration de sa date de terme\n",
       "     -> Géolocalisation pour trouver rapidement une ressource\n",
       "   - Échéances : \n",
       "     - 10/10 Table ronde- journée du numérique ARS\n",
       "     - 11 et 12/10 Journées Réseau Santé Périnatalité de Paris\n",
       "   - KPIs : vs obj 500 à fin novembre: 127 engagées\n",
       "\n",
       "4. Startup OPS :\n",
       "   - Migration de la plateforme vers OVH\n",
       "     - Les environnements sont prêts et testés ✅\n",
       "     - Planifier la prise en charge de l'exploitation des environnements\n",
       "   - Démarche d'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "response = query_engine.query(\"Écris-moi une newsletter sur les dernières infos des startups.\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** To customize Document objects, you can include useful metadata using the `metadata` dictionary on each document. Additionally, you can customize the embedding metadata text by setting the `excluded_embed_metadata_keys` attribute to exclude specific metadata keys from being included in the embedding model. You can also customize the format of the metadata using attributes such as `metadata_separator` and `metadata_template`. Furthermore, you can pass in a service context to specific parts of the pipeline to override the default configuration. This allows you to set different components such as the LLM, embedding model, node parser, and prompt helper according to your requirements, thereby tailoring the behavior of the Document objects to suit your needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"How can I customize Document objects?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** You can customize Document metadata in a few ways. \n",
       "\n",
       "First, you can exclude specific metadata keys from being visible to the LLM (Language Model) by using the `excluded_llm_metadata_keys` attribute. This allows you to exclude certain metadata from being read by the LLM during response synthesis.\n",
       "\n",
       "Second, you can exclude metadata keys from being visible to the embedding model by using the `excluded_embed_metadata_keys` attribute. This is useful if you don't want certain text to bias the embeddings.\n",
       "\n",
       "Additionally, you can customize the format of the metadata using the following attributes:\n",
       "- `metadata_seperator`: controls the separator between each key/value pair of the metadata.\n",
       "- `metadata_template`: controls how each key/value pair is formatted.\n",
       "- `text_template`: controls how the metadata is joined with the text content of the document.\n",
       "\n",
       "You can set the metadata dictionary in the document constructor or after the document is created. You can also set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook.\n",
       "\n",
       "Overall, customizing Document metadata allows you to control what metadata is visible to the LLM and embedding model, as well as the format of the metadata."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"How can I customize Document metadata?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here' we covered a ton of concepts\n",
    "- Node Parsing and Retrievers, specifically the `AutoMergingRetriever` and `HierarchicalNodeParser`\n",
    "- Node post-processors and custom node-postprocessing\n",
    "- Reviewing setting up a `RouterQueryEngine`\n",
    "\n",
    "The full code is available in the `llama_docs_bot` folder in the repo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
